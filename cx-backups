#! /usr/bin/env python3

# A quick and dirty python script for backing up Coralogix alerts and dashboards, optionally
# into a git repo.

# Avi 2025-07-22

import os
import sys
import shutil
import subprocess
import json
import logging
import datetime
import requests
import yaml

from dotenv import load_dotenv

logging.basicConfig(encoding='utf-8', level=logging.INFO)
if 'LOGLEVEL' in os.environ:
  logging.getLogger().setLevel(os.environ['LOGLEVEL'])

if os.path.exists('.env'):
  logging.info("Reading dotenv file at ./.env")
  load_dotenv('.env')

if "CX_API_KEY" in os.environ:
  api_key = os.environ['CX_API_KEY']
  logging.debug(f"Reading CX_API_KEY from environment; got {api_key}")

if "CX_REGION" in os.environ:
  region = os.environ['CX_REGION']
  logging.debug(f"Reading CX_REGION from environment; got {region}")

if 'CX_BACKUPS_GIT_REPO_URL' in os.environ:
  git_repo_url = os.environ['CX_BACKUPS_GIT_REPO_URL']

if 'CX_BACKUPS_WORKDIR' in os.environ:
  workdir = os.environ['CX_BACKUPS_WORKDIR']
elif 'CX_BACKUPS_GIT_REPO_URL' in os.environ:
  workdir = '/tmp/cx-backups/'
else:
  workdir = './cx-backups.d/'

if 'CX_BACKUPS_GIT_COMMIT_MESSAGE' in os.environ:
  git_commit_message = os.environ['CX_BACKUPS_GIT_COMMIT_MESSAGE']
else:
  git_commit_message = 'cx-backups ' + datetime.datetime.now().isoformat()

save_dirs = {
  'dashboards':  './dashboards/',
  'alerts': './alerts/',
  'events2metrics': './events2metrics/',
  'parsing-rules': './parsing-rules/'
}

grpc_endpoints = {
  "AP1": "ng-api-grpc.coralogix.in:443",
  "AP2": "ng-api-grpc.coralogixsg.com:443",
  "AP3": "ng-api-grpc.ap3.coralogix.com:443",
  "EU1": "ng-api-grpc.coralogix.com:443",
  "EU2": "ng-api-grpc.eu2.coralogix.com:443",
  "US1": "ng-api-grpc.coralogix.us:443",
  "US2": "ng-api-grpc.cx498.coralogix.com:443",
}
grpc_endpoint = grpc_endpoints[region]

http_endpoints = {
  "AP1": "https://api.ap1.coralogix.in",
  "AP2": "https://api.ap2.coralogixsg.com",
  "AP3": "https://api.ap3.coralogix.com",
  "EU1": "https://api.coralogix.com",
  "EU2": "https://api.eu2.coralogix.com",
  "US1": "https://api.coralogix.us",
  "US2": "https://api.cx498.coralogix.com",
}
http_endpoint = http_endpoints[region]
write_files = 1

def main():
  if 'CX_BACKUPS_GIT_REPO_URL' in os.environ:

    logging.info(f"git cloning '{git_repo_url}' into '{workdir}'")
    git_clone()

  if not os.path.exists(workdir):
    os.makedirs(workdir)
  os.chdir(workdir)


  # I don't yet have anything useful to do with the list of e2ms or parsing rules, and since the
  # get_* functions do the writing to disk themselves, these just sit there on their own.
  get_e2ms()
  get_parsing_rule_groups()


  alerts_defs = get_alerts_definitions();
  if 'alertDefs' in alerts_defs:
    for alertdef in alerts_defs['alertDefs']:
      alert = get_alert(alertdef['id'])
      filename = alertdef['id'] + '::' + alertdef['alertDefProperties']['name']

      write_json_to_file(alert, 'alerts', filename)
  else:
    logging.info("No alerts found")

  catalog = get_dashboard_catalog();
  db_folders = catalog['folders']
  db_catalog = catalog['dashboards']

  dashboard_summaries = {}
  for dashboard_id in db_catalog:
    board = get_dashboard(dashboard_id)

    summary = summarise_board(board)
    #TODO: Recreate whole hierarchy of folders and subfolder
    if 'folderId' in board['dashboard']:
      folder_id = board['dashboard']['folderId']['value']
      folder_name = db_folders[ folder_id ]['name']
      if not folder_name in dashboard_summaries:
        dashboard_summaries[ folder_name ] = []
      dashboard_summaries[ folder_name ].append(summary)
    else:
      if not '_root_folder' in dashboard_summaries:
        dashboard_summaries[ '_root_folder'] = []
      dashboard_summaries['_root_folder'].append(summary)

    filename = board["dashboard"]["id"] + '::' + board["dashboard"]["name"]
    write_json_to_file(board, 'dashboards', filename)

    write_yaml_to_file(dashboard_summaries, 'dashboards', 'dashboards')

  if 'CX_BACKUPS_GIT_REPO_URL' in os.environ:
    git_commit_push()

# Alerts
def get_alert(id):
  alert = api_request(f"/mgmt/openapi/v3/alert-defs/{id}")
  return alert

def get_alerts_definitions():
  definitions = api_request('/mgmt/openapi/v3/alert-defs?')
  write_json_to_file(definitions, 'alerts', 'definitions')
  return(definitions)

# Dashboards
def get_dashboard(id):
  board = api_request(f"/mgmt/openapi/v1/dashboards/dashboards/{id}")
  return board

def get_dashboard_catalog():
  catalog = api_request('/mgmt/openapi/v1/dashboards/catalog')
  write_json_to_file(catalog, 'dashboards', 'catalog')

  boards = {}
  folders = {}
  for item in catalog["items"]:
    boards[ item['id'] ] = item['name']
    if 'folder' in item and item['folder'] is not None:
      folders[ item['folder']['id'] ] = item['folder']

  catalog = { "dashboards": boards, "folders": folders}
  return catalog

def summarise_board(dashboard):
  summary = {
    "name": dashboard['dashboard']['name'],
    "description": dashboard['dashboard']['description'],
    "sections": []
  }
  for section in dashboard['dashboard']['layout']['sections']:
    s = {}
    if not section is None and 'options' in section and not section['options'] is None and 'custom' in section['options'] and 'name' in section['options']['custom']:
      s['name'] = section['options']['custom']['name']
    if not section is None and 'options' in section and not section['options'] is None and 'custom' in section['options'] and 'description' in section['options']['custom'] and section['options']['custom']['description'] != 'null':
      s['description'] = section['options']['custom']['description']

    s['rows'] = []
    s['kind'] = 'section'
    if 'rows' in section:
      for row in section['rows']:
        r = {}
        r['widgets'] = []
        r['kind'] = 'row'
        if 'widgets' in row:
          for widget in row['widgets']:
            w = {}
            w['kind'] = 'widget'
            w['queries'] = []
            w['title'] = widget['title']
            # There will only be one 'kind' in any given 'widget', but we do a
            # for here rather than needing a list of possible values for it
            for kind in widget['definition']:
              w['type'] = kind
              if 'dataModeType' in widget['definition'] and widget['definition']['dataModeType'] == "DATA_MODE_TYPE_HIGH_UNSPECIFIED":
                w['tier'] = 'frequentsearch'
              if 'dataModeType' in widget['definition'] and widget['definition']['dataModeType'] == "DATA_MODE_TYPE_ARCHIVE":
                w['tier'] = 'archive'
              query_definitions = []

              # Widget types that can have multiple queries (lines, for instance) have a list
              # of query definitions under the key 'queryDefinitions', but those types that can
              # only have one (pie charts) just have a single definition under the key 'query'.
              # Here we normalise those
              if 'queryDefinitions' in widget['definition'][kind]:
                for query in widget['definition'][kind]['queryDefinitions']:
                  query_definitions.append( query )

              if 'query' in widget['definition'][kind]:
                query_definitions.append( widget['definition'][kind]['query'] )

                for queryDef in query_definitions:
                  q = {}
                  for lang in queryDef:
                    if lang == 'dataprime':
                      q['query_language'] = 'dataprime'
                      q['query'] = queryDef['dataprime']['dataprimeQuery']['text']
                      w['queries'].append(q)
                    elif lang =='metrics':
                      q['query_language'] = 'promql'
                      q['query'] = queryDef['metrics']['promqlQuery']['value']
                    elif lang == 'logs':
                      q['query_language'] = 'lucene'
                      q['query'] = queryDef['logs']['luceneQuery']['value']
                    else:
                      logging.error(f"Found unknown query language '{lang}' in dashboard '{dashboard['dashboard']['name']}'")
                      q['error'] = f"Unknown language '{lang}'"
                  w['queries'].append(q)
            r['widgets'].append(w)
        s['rows'].append(r)
    summary['sections'].append(s)
  return summary

# E2M
def get_e2ms():
  list = api_request('/mgmt/openapi/api/v2/events2metrics')
  if 'e2m' in list:
    for e2m in list['e2m']:
      filename = '::'.join([str(e2m['id']), e2m['name']])
      write_json_to_file(list, 'events2metrics', filename)


def api_request(path, data = ()):
  url = http_endpoint + path
  logging.debug('Key: ' + api_key)
  logging.debug(f"Hitting {url}")
  headers = {"Authorization": "Bearer " + api_key}
  response = requests.request("GET", url, headers=headers)
  return json.loads(response.text)

# Parsing Rules

def get_parsing_rule_groups():
  rule_groups = api_request('/mgmt/openapi/api/v1/rulegroups');
  if 'ruleGroups' in rule_groups:
    for rule_group in rule_groups['ruleGroups']:
      filename = '::'.join([str(rule_group['order']), rule_group['id'], rule_group['name']])
      write_json_to_file(rule_groups, 'parsing-rules', filename )
  return(rule_groups);



# Utils
def _grpcurl(location, data = ()):

  command = [
    'grpcurl',
    '-H', f"Authorization: Bearer {api_key}",
  ]
  if ( len(data) > 0 ):
    command.append('-d')
    command.append(json.dumps(data))
  command.append(grpc_endpoint)
  command.append(location)

  logging.debug("command:")
  logging.debug(' '.join(command))

  try:
    result = subprocess.run(command, capture_output=True, encoding="utf-8")
    #TODO catch 403s etc. here
    if len(result.stdout) == 0:
      logging.error(f"Got an empty result from '{grpc_endpoint}' '{location}'")
      return {}
    logging.debug(f"grpcurl got {len(result.stdout)} bytes from '{grpc_endpoint}' '{location}'")
    return json.loads(result.stdout)

  except subprocess.CalledProcessError as err:
    logging.error(f"Error grpcurling '{grpc_endpoint}' '{location}': {err.stderr}")
    exit(255)

def write_json_to_file(data, type, filename, subdir=''):

  if write_files == 0:
    return
  if len(subdir) > 0:
    dirpath = save_dirs[type] + '/' + subdir
  else:
    dirpath = save_dirs[type]

  if not os.path.exists(dirpath):
    os.makedirs(dirpath)

  filename = filename.replace(' ', '_')
  filename = filename.replace('/', '__')

  filepath = dirpath + '/' + filename + '.json'

  with open(filepath, 'w', encoding='utf-8') as f:
    json.dump(data, f, ensure_ascii=False, indent=2, sort_keys=True)


def write_yaml_to_file(data, type, filename, subdir=''):

  if write_files == 0:
    return
  if len(subdir) > 0:
    dirpath = save_dirs[type] + '/' + subdir
  else:
    dirpath = save_dirs[type]

  if not os.path.exists(dirpath):
    os.makedirs(dirpath)

  filename = filename.replace(' ', '_')
  filename = filename.replace('/', '__')

  filepath = dirpath + '/' + filename + '.yaml'

  with open(filepath, 'w', encoding='utf-8') as f:
    yaml.dump(data, f, indent=2, sort_keys=True)

# Git shennanigans
# TODO: Any error handling at all would be nice!

def git_clone():
  if os.path.exists(workdir):
    shutil.rmtree(workdir)
  subprocess.run(['git', 'clone', git_repo_url, workdir])

def git_commit_push():
  subprocess.run(['git', 'add', '.'])
  subprocess.run(['git', 'commit', '-a', '-m', git_commit_message ])
  subprocess.run(['git', 'push'])

main()
